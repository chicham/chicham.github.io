{
  "hash": "cae977c1496a5a96b0a2ddaa0ed9bd6e",
  "result": {
    "markdown": "---\nformat:\n  html:\n    code-fold: true\n  ipynb: default\ntitle: Sharding with jax\n---\n\n# Pre-requisites: Parallelism for deep learning\n\n# Utils functions\n\n::: {#cba8a287 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\n\n\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport keras as K\nfrom pathlib import Path\n\nimport tempfile\nimport shutil\n# jax.distributed.initialize()\n\nN_TRAIN = 8 * 2 ** (10 + 4)\nN_EVAL = 8 * 2**10\nBATCH_SIZE = 32\n\nkey = random.PRNGKey(0)\n\nprint(\"Download dataset\")\n(x_train, y_train), (x_test, y_test) = K.datasets.mnist.load_data()\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = jnp.expand_dims(x_train, -1)\nx_test = jnp.expand_dims(x_test, -1)\n\ninput_shape = 28, 28, 1\n\nprint(\"Make model\")\nmodel = K.Sequential(\n    [\n        K.layers.Input(shape=input_shape),\n        K.layers.Flatten(),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dropout(rate=0.5),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dropout(rate=0.5),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dropout(rate=0.5),\n        K.layers.Dense(128, activation=\"relu\"),\n        K.layers.Dense(10, activation=\"softmax\"),\n    ]\n)\nmodel.summary()\nloss_fn = K.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = K.optimizers.Adam(3e-4)\ntrain_metric = K.metrics.CategoricalAccuracy()\n\n\ndef compute_loss(trainable_variables, non_trainable_variables, metrics_variables, x, y):\n    y_pred, non_trainable_variables = model.stateless_call(\n        trainable_variables, non_trainable_variables, x\n    )\n    loss = loss_fn(y, y_pred)\n    metrics_variables = train_metric.stateless_update_state(\n        metrics_variables, y, y_pred\n    )\n    return loss, (non_trainable_variables, metrics_variables)\n\n\ngrad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n\n\n@jax.jit\ndef train_step(state, data):\n    (\n        trainable_variables,\n        non_trainable_variables,\n        optimizer_variables,\n        metric_variables,\n    ) = state\n    x, y = data\n\n    (loss, (non_trainable_variables, metric_variables)), grads = grad_fn(\n        trainable_variables, non_trainable_variables, metric_variables, x, y\n    )\n    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n        optimizer_variables, grads, trainable_variables\n    )\n\n    return loss, (\n        trainable_variables,\n        non_trainable_variables,\n        optimizer_variables,\n        metric_variables,\n    )\n\nlogs_dir = Path(\"logs/\").resolve()\nlogs_dir.mkdir(exist_ok=True)\n\ntrainable_variables = model.trainable_variables\nnon_trainable_variables = model.non_trainable_variables\n\noptimizer.build(trainable_variables)\noptimizer_variables = optimizer.variables\nmetrics_variables = train_metric.variables\n\nstate = (\n    trainable_variables,\n    non_trainable_variables,\n    optimizer_variables,\n    metrics_variables,\n)\n\nx_train = x_train[2 * BATCH_SIZE :]\ny_train = y_train[2 * BATCH_SIZE :]\n\ndef run(state, x_train, y_train):\n    acc_loss = 0\n    for step in range(0, x_train.shape[0], BATCH_SIZE):\n        data = x_train[step : step + BATCH_SIZE], y_train[step : step + BATCH_SIZE]\n        with jax.profiler.TraceAnnotation(\"train_step\"):\n            loss, state = train_step(state, data)\n            loss.block_until_ready()\n            acc_loss += loss\n        if step % 100 == 0:\n            *_, metrics_variables = state\n            for variable, value in zip(train_metric.variables, metrics_variables):\n                variable.assign(value)\n            print(f\"Acc: {train_metric.result()}\")\n            print(f\"Loss: {acc_loss / (step + 1)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownload dataset\nMake model\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n└─────────────────────────────────┴───────────────────────────┴────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">200,842</span> (784.54 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">200,842</span> (784.54 KB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n```\n:::\n:::\n\n\n# Experiments\n## Baseline run on 1 device\n\n::: {#376f6f72 .cell execution_count=2}\n``` {.python .cell-code}\nbaseline_dir = logs_dir / \"baseline\"\n\nif not baseline_dir.exists():\n    with tempfile.TemporaryDirectory(prefix=\"sharding_jax_\") as tmpdir:\n        with jax.profiler.trace(tmpdir):\n            run(state, x_train, y_train)\n        shutil.move(tmpdir, baseline_dir)\n```\n:::\n\n\n---\njupyter:\n  jupytext:\n    cell_metadata_filter: '-all'\n    formats: 'qmd,py:percent'\n    main_language: python\n    text_representation:\n      extension: .qmd\n      format_name: quarto\n      format_version: '1.0'\n      jupytext_version: 1.15.2\n  kernelspec:\n    display_name: Python 3\n    language: python\n    name: python3\n---\n",
    "supporting": [
      "sharding_jax_files/figure-ipynb"
    ],
    "filters": []
  }
}
