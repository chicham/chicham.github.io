{
  "hash": "90fc954de9d5412b8d75cb3063083b14",
  "result": {
    "markdown": "---\ntitle: \"Sharding with jax\"\nformat:\n  html:\n    code-fold: true\n  ipynb: default\nexecute:\n  eval: false\n  echo: fenced\ndraft : true\n---\n\n# Sharding with jax\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chicham/chicham.github.io/blob/gh-pages/posts/sharding-with-jax/sharding_jax.ipynb)\n\n## Links\n\n- [Tutorial ICML 2022](https://sites.google.com/view/icml-2022-big-model/home)\n- [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\n- [https://github.com/google-research/big_vision](Big vision) the idea of the sharding class\n\n## Pre-requisites\n\n- Train a 8 layers model\n- Generate random data cube\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\nfrom jax.random import PRNGKe\nimport jax.numpy as jnp\n\nimport keras_core as K\nimport datetime\n\n\nN_TRAIN = 8*2**(10+4)\nN_EVAL = 8*2**10\n\nkey = PRNGKey(0)\n\n(x_train, y_train), (x_test, y_test) = K.datasets.mnist.load_data()\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = jnp.expand_dims(x_train, -1\nx_test = jnp.expand_dims(x_test, -1)\n\ninput_shape = 28, 28, 1\n\nmodel = K.Sequential(\n  [\n    K.layers.Flatten(input_shape),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dropout(rate=.5),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dropout(rate=.5),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dropout(rate=.5),\n    K.layers.Dense(128, activation=\"relu\"),\n    K.layers.Dense(10, activation=\"softmax\"),\n  ]\n)\n\nmodel.summary()\n\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\",\n    optimizer=K.optimizers.Adam(0.001),\n    metrics=[\"accuracy\"]\n)\n\n```\n\n````\n:::\n\n\n## Methods\n\n### Baseline\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nbaseline_logs = \"logs/baseline/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\nbaseline_callback = K.callbacks.TensorBoard(\n  log_dir = baseline_logs,\n  histogram_freq = 1,\n  profile_batch = '500,520'\n)\n\n\nmodel.fit(\n  x_train,\n  y_train,\n  epochs=2,\n  validation_split=0.20,\n  callbacks = [baseline_callback])\n```\n\n````\n:::\n\n\n- Data parallelism\n  - Data parallelism with all reduce\n- Model Parallelism\n  - Device placement\n    - issue: each modules must wait for the computation of the previous one\n    - Micro batches\n      - GPipe\n      - 1F1B\n      - Interleaved 1F1B (megatron)\n        - Split layers between multiple devices\n    - Async pipelines (won't cover this)\n  - Split parameters ( intra-op parallelism)\n    - Megatron-LM\n    - ZeRO\n\n",
    "supporting": [
      "sharding_jax_files"
    ],
    "filters": [],
    "includes": {}
  }
}
